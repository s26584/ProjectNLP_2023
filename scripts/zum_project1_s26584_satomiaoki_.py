# -*- coding: utf-8 -*-
"""ZUM_project1_s26584_SatomiAoki_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16JYVu7UH-X7pmbaF3kCsIIH4FN0oFq5U

# STAGE 1: DATA COLLECTION
### 1A â€“ data in English

Data acquisition concerns the collection of tweets. Each person scraps tweets (about 200k) to create a dataset for
further processing. Tweets should be about current events, such as the war, NATO etc.

1. Adding class labels: Collected data is not tagged as positive/negative/neutral.
    - a. Select the number of target classes (2 or 3 if we include neutral).
    - b. Clean data and remove stopwords
    - c. Create word embeddings for vectorized representation of words similar in meaning // OR we use
pretrained model for language of choice
    - d. Use K-MEANS to create clusters and use k=2 or k=3 depending on the number of target classes
    - e. Based on clusters tag data and manually fix clusters if necessary

It is a good idea to limit the number of words as much as possible and possibly manually tag some of them too.

2. Data cleaning: normalisation, special characters removal, punctuation, URL, emails, duplicates, lowercase
text and choose type of tokenizer. *NOTICE*: this stage is necessary BEFORE the creation of word embeddings.
"""

# Functions are implemented referencing ZUM class materials.

!pip install snscrape

!pip install unidecode -q

import csv
import pandas as pd
import snscrape.modules.twitter as sntwitter
from google.colab import drive

drive.mount('/content/drive')

from unidecode import unidecode

keyword = 'war Ukraine -is:retweet'
tweets = 0
limits = 200000
header = ['date', 'username', 'content']

with open('drive/MyDrive/ZUM/project1/tweets_en.csv', 'w') as f:
    w = csv.writer(f, quoting=csv.QUOTE_ALL)
    w.writerow(header)

    for tweet in sntwitter.TwitterSearchScraper(keyword).get_items():
      if tweets == limits:
        break
      else:
        if tweet.lang=='en':
          w.writerow([tweet.date, tweet.user.username, tweet.rawContent])
          tweets += 1

df = pd.read_csv('drive/MyDrive/ZUM/project1/tweets_en.csv')
print('number of tweets:', df.shape[0])
print('\n')
print(df.head(5))

"""a. Select the number of target classes (2 or 3 if we include neutral)."""

target_classes = 2

"""b. Clean data and remove stopwords"""

# extract only tweets ('content' column)
import pandas as pd

tweet = df['content']
print(tweet.head(5))

import re 
from re import sub
import nltk

from nltk import word_tokenize
#from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

stopwords = nltk.corpus.stopwords.words('english')

nltk.download('stopwords')
nltk.download('wordnet')

def clean_data(tweet):

  # convert texts into lower case
  text = tweet.lower()
  text = str(text)

  # remove URLs, ticker symbols, usernames, apostrophe, single words
  text = sub(r'https?:\/\/[a-zA-Z0-9@:%._\/+~#=?&;-]*', ' ', text)
  text = sub(r'\$[a-zA-Z0-9]*', ' ', text)
  text = sub(r'\@[a-zA-Z0-9]*', ' ', text)
  text = sub(r'\[^a-zA-Z\']', ' ', text)

  text = sub(r"[^A-Za-z0-9^,!?.\/'+]", " ", text) 
  text = sub(r'[0-9]', ' ', text)
  text = sub(r"\+", " plus ", text) 
  text = sub(r",", " ", text) 
  text = sub(r"_", " ", text) 
  text = sub(r"\.", " ", text) 
  text = sub(r"!", " ! ", text) 
  text = sub(r"\?", " ? ", text) 
  text = sub(r"'", " ", text) 
  text = sub(r"/", " ", text) 
  text = sub(r":", " : ", text) 
  text = sub(r"\s{2,}", " ", text)

  # remove short word 
  text = ' '.join([t for t in text.split() if len(t)>1])

  # Lemmatize and remove stop words
  lemma = WordNetLemmatizer()

  text = ' '.join([lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(text) if x not in stopwords])
  text = [lemma.lemmatize(x, nltk.corpus.reader.wordnet.VERB) for x in nltk.wordpunct_tokenize(text) if x not in stopwords]
  return text

cleaned_tweet = tweet.apply(clean_data)

cleaned_tweet.head()

"""c. Create word embeddings for vectorized representation of words similar in meaning // OR we use pretrained model for language of choice"""

import gensim
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer 

from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import gensim.downloader as api

from time import time
import multiprocessing

import numpy as np

from gensim.models.phrases import Phrases, Phraser

w2v_model = Word2Vec(min_count=3,
                     window=4,
                     vector_size=300,
                     sample=1e-5, 
                     alpha=0.03, 
                     min_alpha=0.0007, 
                     negative=20,
                     workers=multiprocessing.cpu_count()-1)

start = time()

w2v_model.build_vocab(cleaned_tweet, progress_per=50000)

print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))

start = time()

w2v_model.train(cleaned_tweet,
                total_examples=w2v_model.corpus_count,
                epochs=30,
                report_delay=1)

print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))

w2v_model.init_sims(replace=True)

# save model
w2v_model.save("word2vec.model")

# Detect bigrams with gensim's Phraser module.

sent = [row for row in cleaned_tweet]
phrases = Phrases(sent, min_count=1, progress_per=50000)
bigram = Phraser(phrases)
sentences = bigram[sent]
sentences[1]

# export cleaned dataset
file_export = cleaned_tweet.copy()
file_export = file_export.apply(lambda x: ' '.join(bigram[x]))
file_export.to_csv('drive/MyDrive/ZUM/project1/cleaned_dataset.csv', index=False)

cleaned_dataset = pd.read_csv(r'drive/MyDrive/ZUM/project1/cleaned_dataset.csv')
print(len(cleaned_dataset))
print(cleaned_dataset.head(10))

# check similarity of word
w2v_model.wv.most_similar(positive=['peace'])

"""d. Use K-MEANS to create clusters and use k=2 or k=3 depending on the number of target classes"""

from sklearn.cluster import KMeans

word_vectors = Word2Vec.load("word2vec.model").wv

model = KMeans(n_clusters=target_classes, # positive and negative
               max_iter=1000,
               random_state=42,
               n_init=50)
model.fit(X=word_vectors.vectors.astype('double'))

"""e. Based on clusters tag data and manually fix clusters if necessary"""

# check words in each class
word_vectors.similar_by_vector(model.cluster_centers_[0], 
                               topn=10, 
                               restrict_vocab=None)

# check words in each class
word_vectors.similar_by_vector(model.cluster_centers_[1], 
                               topn=10, 
                               restrict_vocab=None)

# assign centroids to respective sentiment labels
positive_cluster_index = 0
positive_cluster_center = model.cluster_centers_[positive_cluster_index]
negative_cluster_center = model.cluster_centers_[1-positive_cluster_index]

words = pd.DataFrame(word_vectors.index_to_key)
words.columns = ['words']
words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])
words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))
words.cluster = words.cluster.apply(lambda x: x[0])
words['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]
words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)
words['sentiment_coeff'] = words.closeness_score * words.cluster_value

words.head(10)

words[['words', 'sentiment_coeff']].to_csv('drive/MyDrive/ZUM/project1/sentiment_dictionary.csv', index=False)

# labeling data

from IPython.display import display
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score

final_file = pd.read_csv('drive/MyDrive/ZUM/project1/cleaned_dataset.csv')
sentiment_map = pd.read_csv('drive/MyDrive/ZUM/project1/sentiment_dictionary.csv')
sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))

file_weighting = final_file.copy()
tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)
file_weighting.content = file_weighting.content.fillna(' ')
tfidf.fit(file_weighting.content)
features = pd.Series(tfidf.get_feature_names_out())
transformed = tfidf.transform(file_weighting.content)

def create_tfidf_dictionary(x, transformed_file, features):
    '''
    create dictionary for each input sentence x, where each word has assigned its tfidf score
    
    x - row of dataframe, containing sentences, and their indexes,
    transformed_file - all sentences transformed with TfidfVectorizer
    features - names of all words in corpus used in TfidfVectorizer

    '''
    vector_coo = transformed_file[x.name].tocoo()
    vector_coo.col = features.iloc[vector_coo.col].values
    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))
    return dict_from_coo

def replace_tfidf_words(x, transformed_file, features):
    '''
    replacing each word with it's calculated tfidf dictionary with scores of each word
    x - row of dataframe, containing sentences, and their indexes,
    transformed_file - all sentences transformed with TfidfVectorizer
    features - names of all words in corpus used in TfidfVectorizer
    '''
    dictionary = create_tfidf_dictionary(x, transformed_file, features)   
    return list(map(lambda y:dictionary[f'{y}'], x.content.split()))

# %%time
replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)

# Replacing words in sentences with their sentiment score

def replace_sentiment_words(word, sentiment_dict):
    '''
    replacing each word with its associated sentiment score from sentiment dict
    '''
    try:
        out = sentiment_dict[word]
    except KeyError:
        out = 0
    return out

replaced_closeness_scores = file_weighting.content.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))

# Merging both previous steps and getting the predictions:

# The dot product of such 2 sentence vectors indicates whether overall sentiment is positive or negative 
# (if the dot product is positive, the sentiment is considered positive, otherwise negative).

replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.content]).T
replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'tweet']
replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)
replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')
# replacement_df['sentiment'] = [1 if i==1 else 0 for i in replacement_df.sentiment]

replacement_df.sample(10)

replacement_df.groupby(['prediction']).count()

replacement_df = replacement_df.rename(columns={'prediction': 'label'})
labeled_data = replacement_df.copy()
labeled_data.to_csv('drive/MyDrive/ZUM/project1/labeled_dataset.csv', index=False)

labeled_data.columns

# visualize words in each cluster
from wordcloud import WordCloud

positive_words = ' '.join(labeled_data[labeled_data['label']==0]['tweet'])
wc = WordCloud(max_words=1000, width=800, height=400, collocations=False).generate(positive_words)
wc.to_image()

# visualize words in each cluster
from wordcloud import WordCloud

negative_words = ' '.join(labeled_data[labeled_data['label']==1]['tweet'])
wc = WordCloud(max_words=1000, width=800, height=400, colormap='PuRd', collocations=False).generate(negative_words)
wc.to_image()

# check the data distribution of each label
# plot the balance
import seaborn as sns

sns.countplot(x='label', data=labeled_data)

"""# ETAP 2: CLASSIC ML
Choose 3 models to fit data and present the results with confusion matric and roc curve. Just as in class. 
"""

import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression

import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report

import pickle

from google.colab import drive
drive.mount('/content/drive')

DATA_FILE_PATH = 'drive/MyDrive/ZUM/project1/labeled_dataset.csv'
labeled_df = pd.read_csv(DATA_FILE_PATH)
labeled_df.head()

X = labeled_df.tweet
Y = labeled_df.label

# train:test = 9:1
# 'stratify' preserves the proportion of target as in original dataset
X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    Y, 
                                                    test_size=0.1, 
                                                    stratify=Y, 
                                                    random_state=1)

vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)
vectoriser.fit(X_train)
print('Number of feature words: ', len(vectoriser.get_feature_names_out()))

X_train = vectoriser.transform(X_train)
X_test  = vectoriser.transform(X_test)

def plot_performance(model):
  y_pred = model.predict(X_test)
  print(classification_report(y_test, y_pred))
  
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
  print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
  
  confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['positive', 'negative'])

  cm_display.plot()
  plt.show()

# plot roc curve
def plot_roc_curve(modelname):
  pred_y = modelname.predict(X_test)

  fpr, tpr, thresh = metrics.roc_curve(y_test, pred_y[::,1])

  auc = metrics.auc(fpr, tpr)

  plt.plot(fpr, tpr, label='ROC curve (area = %.2f)' %auc)
  plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')
  plt.title('ROC curve')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.grid()
  plt.legend()
  plt.show()

# save model with Pickle
def save_model_pkl(filename, modelname):
  with open(filename, 'wb') as file:
    pickle.dump(modelname, file)

"""SVM (Linear SVC)"""

# 0: positive, 1: negative

LinearSVCmodel = LinearSVC()
LinearSVCmodel.fit(X_train, y_train)

plot_performance(LinearSVCmodel)

# save model
save_model_pkl('drive/MyDrive/ZUM/project1/linearsvc_model.pkl', LinearSVCmodel)

"""Logistic Regression"""

Logisticmodel = LogisticRegression(max_iter = 1000)
Logisticmodel.fit(X_train, y_train)

plot_performance(Logisticmodel)

# save model
save_model_pkl('drive/MyDrive/ZUM/project1/logi_model.pkl', Logisticmodel)

"""BernoulliNB Model"""

BNBmodel = BernoulliNB(alpha = 2)
BNBmodel.fit(X_train, y_train)

plot_performance(BNBmodel)

# save model
save_model_pkl('drive/MyDrive/ZUM/project1/bernoulli_model.pkl', BNBmodel)

# plot roc curve
# https://www.statology.org/plot-multiple-roc-curves-python/ 
y_pred_ls = LinearSVCmodel.fit(X_train, y_train)._predict_proba_lr(X_test)[:,1] 
fpr1, tpr1, _ = metrics.roc_curve(y_test,  y_pred_ls)

y_pred_logi = Logisticmodel.predict_proba(X_test)[::,1]
fpr2, tpr2, _ = metrics.roc_curve(y_test,  y_pred_logi)

y_pred_bnb = BNBmodel.predict_proba(X_test)[::,1]
fpr3, tpr3, _ = metrics.roc_curve(y_test,  y_pred_bnb)

plt.plot(fpr1, tpr1, label='Linear SVC')
plt.plot(fpr2, tpr2, label='Logistic Regression')
plt.plot(fpr3, tpr3, label='BernoulliNB')
plt.plot([0, 1], [0, 1], "k--", label="chance level (AUC = 0.5)")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend()
plt.show()

"""# ETAP 3: NEURAL MODEL
Choose type of neural network to train, and through validation decide on the best set of parameters. It is not enough
to just build a model and get results. Fine-tuning is necessary too. 
"""

import pandas as pd

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import optimizers
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical

import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

DATA_FILE_PATH = 'drive/MyDrive/ZUM/project1/labeled_dataset.csv'
labeled_df = pd.read_csv(DATA_FILE_PATH)
labeled_df.head()

X = labeled_df.tweet
Y = labeled_df.label

# train:test = 9:1
# 'stratify' preserves the proportion of target as in original dataset
X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    Y, 
                                                    test_size=0.1, 
                                                    stratify=Y, 
                                                    random_state=1)

# train:validation = 4:1
X_train, X_val, y_train, y_val = train_test_split(X_train, 
                                                  y_train, 
                                                  test_size=0.2, 
                                                  random_state=1)

# train:val:test = 72:18:10
print("X_train: ", X_train.shape) 
print("X_test: ", X_test.shape) 
print("X_val: ", X_val.shape) 
print("Y_train: ", y_train.shape)
print("Y_test: ", y_test.shape)
print("Y_val: ", y_val.shape)

from tensorflow.keras.preprocessing.text import Tokenizer

_tokenizer = Tokenizer()
_tokenizer.fit_on_texts(X)

X_train_converted = _tokenizer.texts_to_sequences(X_train)
X_val_converted = _tokenizer.texts_to_sequences(X_val)
X_test_converted = _tokenizer.texts_to_sequences(X_test)

print('X_train_converted: ')
print(X_train_converted[0])
print('X_val_converted: ')
print(X_val_converted[0])
print('X_test_converted: ')
print(X_test_converted[0])

print('X_train_converted')
print(_tokenizer.sequences_to_texts(X_train_converted)[0])
print('X_val_converted')
print(_tokenizer.sequences_to_texts(X_val_converted)[0])
print('X_test_converted')
print(_tokenizer.sequences_to_texts(X_test_converted)[0])

max_length = len(max(X, key=len)) 
max_length

from tensorflow.keras.utils import pad_sequences

X_train_padded = pad_sequences(X_train_converted, maxlen=max_length, padding='post',truncating='post')
X_val_padded = pad_sequences(X_val_converted, maxlen=max_length, padding='post',truncating='post')
X_test_padded = pad_sequences(X_test_converted, maxlen=max_length, padding='post',truncating='post')

print(f'train padded shape: {X_train_padded.shape}')
print(f'validation padded shape: {X_val_padded.shape}')
print(f'test padded shape: {X_test_padded.shape}')

print(y_train[:3])
print(y_val[:3])
print(y_test[:3])

y_train_encoded = to_categorical(y_train)
y_val_encoded = to_categorical(y_val)
y_test_encoded = to_categorical(y_test)

y_train_encoded[:5]
y_val_encoded[:5]
y_test_encoded[:5]

word_length = len(_tokenizer.word_index) + 1

print(f'Max length: {max_length}')
print(f'Vocabulary size: {word_length}')

# build a cnn model with fine-tuning
model_cnn = tf.keras.Sequential([
    layers.Embedding(word_length, 100, input_length=max_length),
    layers.Conv1D(filters=32, kernel_size=1, activation='relu'),
    layers.MaxPooling1D(pool_size=2, padding='same'),
    layers.Flatten(),
    layers.Dense(10, activation='relu'),
    layers.Dropout(0.07),
    layers.Dense(2, activation='softmax')
])

model_cnn.summary()

# early stopping: stop if model does not improve precision for 3 epochs
# (please change the number at the end of "val_precision_" depends on your training status)
# model checkpoint: save model when validation cost (val_loss) is the lowest
 
callbacks_list = [
    keras.callbacks.EarlyStopping(
        monitor="val_precision_2",
        patience=3, 
    ),
    keras.callbacks.ModelCheckpoint(
        filepath="checkpoint_path.keras",
        monitor="val_loss",
        save_best_only=True, 
    )
]

optimizer = keras.optimizers.Adam(learning_rate=0.001)

precision = keras.metrics.Precision()
recall = keras.metrics.Recall()

model_cnn.compile(
    optimizer = optimizer, 
    loss = "categorical_crossentropy", 
    metrics=[precision, recall])

history_cnn = model_cnn.fit(
     X_train_padded, 
     y_train_encoded,
     epochs = 10,
     callbacks=callbacks_list,
     validation_data=(X_val_padded, y_val_encoded), 
     verbose=True)

!pip install plot_keras_history

# plot results
from plot_keras_history import show_history, plot_history
import matplotlib.pyplot as plt

show_history(history_cnn)
plot_history(history_cnn)
plt.close()

# test model performance
test_loss, test_precision, test_recall = model_cnn.evaluate(X_test_padded, y_test_encoded)
print(f"Test loss: {test_loss:.3f}") 
print(f"Test precision: {test_precision:.3f}")
print(f"Test recall: {test_recall:.3f}")
test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)
print(f"Test f1 score: {test_f1:.3f}")

# plot test confusion matrix
y_pred = model_cnn.predict(X_test_padded)
y_pred_classes = np.argmax(y_pred, axis = 1)
y_true = np.argmax(y_test_encoded, axis = 1)
confusion_mtx = confusion_matrix(y_true, y_pred_classes)
f,ax = plt.subplots(figsize = (5, 3))
sns.heatmap(confusion_mtx, annot=True, linewidths=0.01, cmap="Reds",
            linecolor = "gray", fmt = ".0f", ax=ax)
plt.xlabel("Predicted label")
plt.ylabel("True Label")
plt.title("Confusion matrix")
plt.show()

# save model as JSON
model_cnn_json = model_cnn.to_json()
with open("drive/MyDrive/ZUM/project1/model_cnn.json", "w") as json_file:
    json_file.write(model_cnn_json)

# serialize weights to HDF5
model_cnn.save_weights("drive/MyDrive/ZUM/project1/model_cnn.h5")
print("Saved model to disk.")

"""# ETAP 4: LANGUAGE MODEL
The last stage is to use selected language model, e.g. BERT, to create a sentiment analysis classifier. 
"""

# BERT Language Model

!pip install -U transformers

!pip install -U datasets

import pandas as pd
from datasets import Dataset
import csv

import transformers
transformers.__version__

from google.colab import drive
drive.mount('/content/drive')

DATA_FILE_PATH = 'drive/MyDrive/ZUM/project1/labeled_dataset.csv'
labeled_df = pd.read_csv(DATA_FILE_PATH)
labeled_df.head()

header = ["tweet", "label"]
temp_ds = pd.DataFrame(data=labeled_df, columns=header)
temp_ds.sample(5)

dataset_ = Dataset.from_pandas(temp_ds)
dataset = dataset_.train_test_split(0.2)

print(dataset)

dataset['train']

from transformers import AutoTokenizer

model_checkpoint = 'distilbert-base-uncased'
batch_size = 128

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

tokenizer('Today is Friday.')

def process(x):
  return tokenizer(x['tweet'])

train_ds = dataset['train'].map(process)
test_ds = dataset['test'].map(process)

train_ds

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

num_labels = 2
lang_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

!pip install --upgrade accelerate

# NameError: name 'PartialState' is not defined in TrainingArguments
!pip uninstall -y transformers accelerate
!pip install transformers accelerate

args = TrainingArguments(
    f'{model_checkpoint}_sentiment_analysis',
    evaluation_strategy = 'epoch',
    save_strategy = 'epoch',
    learning_rate = 2e-5,
    per_device_train_batch_size = batch_size,
    per_device_eval_batch_size = batch_size,
    num_train_epochs = 3,
    weight_decay = 0.01,
    load_best_model_at_end = True,
    metric_for_best_model = 'loss'
)

from datasets import load_metric
import numpy as np

metric = load_metric('f1', 'precision')

def compute_metrics(eval_preds):
  logits, labels = eval_preds
  predictions = np.argmax(logits, axis=-1)
  return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    lang_model,
    args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.evaluate([train_ds[0]])

trainer.train()

# evaluate the trained model with test dataset
trainer.evaluate(test_ds)

import pickle

# save the model to disk
trained_model = trainer.model
filename = 'drive/MyDrive/ZUM/project1/lang_model.sav'
pickle.dump(trained_model, open(filename, 'wb'))

#Load the model
loaded_model = pickle.load(open(filename, 'rb'))

import torch

def txt_sentiment_classifier(txt):
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  inputs = tokenizer(txt, return_tensors='pt')
  input_ids = inputs['input_ids'].to(device)
  attention_mask = inputs['attention_mask'].to(device)
  with torch.no_grad():
    output = loaded_model(input_ids=input_ids, attention_mask=attention_mask)
    logits = output.logits
    predictions = torch.argmax(logits, dim=-1)
  if predictions.item() == 0:
    print(txt, '\n-> negative\n')
  else:
    print(txt, '\n-> positive\n')

txt_list = [
  'I strongly disagree with the war. Everyone deserves peace.',
  'Ukrain has all the support, we gonna win',
  ' :( My heart is totally broken...'
]

for txt in txt_list:
  txt_sentiment_classifier(txt)